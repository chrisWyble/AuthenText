{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7188d6e4-0aee-4c59-ab89-d577cc4be4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import boto3\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "from binoculars import Binoculars\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c13ddc39-a553-4595-9064-1a47e6415fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(f\"../credentials.env\")\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv('aws_access_key_id'),\n",
    "    aws_secret_access_key=os.getenv('aws_secret_access_key')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead196d9-49f6-488c-90fb-5002e5a766a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body_from_s3(\n",
    "    bucket_name: str,\n",
    "    file_key: str,\n",
    "):\n",
    "\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    body = obj['Body']\n",
    "    return body\n",
    "\n",
    "def stream_csv_from_s3(\n",
    "    body,\n",
    "    chunk_size: int = 1_000_000,\n",
    "):\n",
    "    newline = '\\n'.encode()\n",
    "    partial_chunk = b''\n",
    "\n",
    "    while True:\n",
    "        # Combine previous unfinished chunk with current.\n",
    "        chunk = partial_chunk + body.read(chunk_size)\n",
    "\n",
    "        # Exit if no more content\n",
    "        if chunk == b'':\n",
    "            break\n",
    "        df = None\n",
    "        # Find last newline tag\n",
    "        last_newline = chunk.rfind(newline)\n",
    "        while df is None:\n",
    "          try:\n",
    "            if last_newline == -1:\n",
    "                partial_chunk = chunk\n",
    "                continue\n",
    "            result = chunk[:last_newline + 1].decode('utf-8')\n",
    "            partial_chunk = chunk[last_newline + 1:]\n",
    "            # Convert the chunk to a DataFrame\n",
    "            df = pd.read_csv(StringIO(result))\n",
    "          except Exception as e:\n",
    "            last_newline = chunk.rfind(newline, 0, last_newline)\n",
    "            continue\n",
    "        yield last_new_linedf\n",
    "\n",
    "    if partial_chunk:\n",
    "        result = partial_chunk.decode('utf-8')\n",
    "        df = pd.read_csv(StringIO(result))\n",
    "        yield -1, df\n",
    "\n",
    "def read_csv_from_s3(\n",
    "    bucket_name: str,\n",
    "    file_key: str,\n",
    "):\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    body = obj['Body']\n",
    "    data = body.read().decode('utf-8')\n",
    "    \n",
    "    # Use StringIO to convert the string data to a pandas-readable buffer\n",
    "    data_buffer = StringIO(data)\n",
    "\n",
    "    # Read the data into a pandas DataFrame\n",
    "    df = pd.read_csv(data_buffer)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf38185e-bfa6-4ec2-9c38-9e60e05311be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/softwares/python3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffe12c67cf1445f963d06bf2139437d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/softwares/python3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f226e0b09e4b4e0684ef3575e5814bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FalconForCausalLM were not initialized from the model checkpoint at vilsonrodrigues/falcon-7b-instruct-sharded and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "binoculars = Binoculars()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c5df1-9660-4fbe-bb23-7bd0bf12e9fd",
   "metadata": {},
   "source": [
    "# Evaluate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d82816-e43d-4254-845b-6ea9dab3700c",
   "metadata": {},
   "source": [
    "## Evaluation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a254ba-f1e0-43ef-b7bf-52312b9d97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    " # 'dataset1_test.csv', # Completed\n",
    " # 'dataset1_train.csv', # Completed\n",
    " # 'dataset2_test.csv', # Completed\n",
    " # 'dataset2_train.csv', # Completed\n",
    " # 'dataset3_test.csv', # Completed\n",
    " # 'dataset3_train.csv', # Completed\n",
    " # 'dataset4_test.csv', # Completed\n",
    " # 'dataset4_train.csv', # Completed\n",
    " # 'dataset5_test.csv', # Completed\n",
    " # 'dataset5_train.csv', # Completed\n",
    " # 'dataset6_test.csv', # Completed\n",
    " # 'dataset6_train.csv',\n",
    "    'machine_generated.csv',\n",
    "]\n",
    "\n",
    "bucket_name = 'training-essays'\n",
    "batch_resume_index = 1401\n",
    "batch_size = 4\n",
    "\n",
    "chunk_size = 500_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c05acb9-de71-45ac-8818-d1db73a5fa22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a97acc58-311d-4972-b319-f2f17822348a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for machine_generated.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 350/350 [10:22<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file_number, file in enumerate(filenames):\n",
    "    df = read_csv_from_s3(bucket_name, file)\n",
    "    df_size = len(df)\n",
    "    \n",
    "    num_batches = math.ceil(len(df) / batch_size)\n",
    "    \n",
    "    print(f'Running evaluation for {file}.')\n",
    "    for i, start_idx in tqdm(enumerate(range((batch_resume_index - 1) * batch_size, len(df), batch_size)), total=num_batches - batch_resume_index + 1):\n",
    "        end_idx = min(start_idx + batch_size, df_size)\n",
    "        batch_df = df.iloc[start_idx: end_idx]\n",
    "        \n",
    "        scores = binoculars.compute_score(batch_df['text'].to_list())\n",
    "\n",
    "        batch_df = batch_df.copy(deep=True)\n",
    "        batch_df.loc[:, 'binocular_score'] = scores\n",
    "        batch_df.to_csv('temp_prediction_scores.csv', index=False, header=(i + batch_resume_index - 1 == 0), mode='w' if i + batch_resume_index - 1 == 0 else 'a')\n",
    "        \n",
    "        try:\n",
    "          name = file.split('.')[0]\n",
    "          s3.upload_file('temp_prediction_scores.csv', bucket_name, f'evaluations/{name}_predictions.csv')\n",
    "        except Exception as e:\n",
    "          print(f\"\\nError occurred: {e}. \\n\\nResume run with {file} and 'batch_resume_index' set to {i + batch_resume_index}.\")\n",
    "          raise\n",
    "        # Clear GPU memory if using PyTorch\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        del batch_df, scores\n",
    "        gc.collect()\n",
    "    del df\n",
    "    os.remove('temp_prediction_scores.csv')\n",
    "print('Evaluation Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b1ced-da0a-4c88-86da-c2167d70c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_csv_from_s3(bucket_name, 'evaluations/dataset1_test_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cedca-f516-4517-8cff-1d360d4d3016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
